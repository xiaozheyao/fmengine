model:
  architecture: llama
  hidden_size: 2048
  n_layers: 22
  n_heads: 32
  intermediate_dim: 5632
  n_kv_heads: 4
  vocab_size: 32000
  multiple_of: 256
  ffn_dim_multiplier: 256.0
  norm_eps: 1.0e-05
  rope_theta: 10000.0
  max_seq_len: 2048
  depth_init: true
  norm_type: fused_rmsnorm
  activation: silu
  attn_dropout: 0.0
  torch_dtype: float32
  initializer_range: 0.02
  attn_impl: fa
