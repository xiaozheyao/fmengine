model:
  architecture: llama
  attention_bias: false
  bos_token_id: 1
  eos_token_id: 2
  hidden_act: silu
  dim: 2048
  initializer_range: 0.02
  intermediate_size: 4096
  max_position_embeddings: 2048
  n_heads: 32
  n_layers: 22
  n_kv_heads: 4
  rms_norm_eps: 1.0e-05
  rope_theta: 500000
  tie_word_embeddings: false
  torch_dtype: bfloat16
  use_cache: true
  ffn_dim_multiplier: 1.3
  multiple_of: 1024
  vocab_size: 32000

tokenizer:
  pretrained: meta-llama/Meta-Llama-3.1-8B-Instruct

checkpoint:
  ckpt_dir: .local/ckpts

training:
  gc_freq: 1000
  dp_degree: 1
  tp_degree: 1
  pp_degree: 1
  enable_loss_parallel: true
  data_parallel_type: fsdp
  dump_folder: ./outputs