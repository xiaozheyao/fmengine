model:
  architecture: llama
  attention_bias: false
  bos_token_id: 1
  eos_token_id: 2
  activation: silu
  hidden_size: 2048
  initializer_range: 0.02
  intermediate_dim: 5632
  max_seq_len: 2048
  n_heads: 32
  n_layers: 1
  n_kv_heads: 8
  rms_norm_eps: 1.0e-05
  rope_theta: 10000
  tie_word_embeddings: false
  torch_dtype: float32
  use_cache: true
  ffn_dim_multiplier: 1.3
  multiple_of: 1024
  vocab_size: 32000
  norm_type: fused_rmsnorm
  attn_impl: torch

tokenizer:
  tokenizer_type: huggingface
  tokenizer_name_or_path: meta-llama/Llama-2-7b-chat-hf

checkpoint:
  ckpt_dir: .local/checkpoints-micro/
  create_seed_checkpoint: false
  interval: 2000
  async_mode: async

training:
  gc_freq: 1000
  dp_replicate: 2
  dp_shard: 1
  tp_degree: 1
  pp_degree: 1
  accumulate_steps: 1
  enable_loss_parallel: true
  data_parallel_type: fsdp
  dump_folder: ./outputs
  train_steps: 100000
  warmup_steps: 20000
  ac_mode: selective
  selective_ac_option: "3"
  compile: false # tbd
  mixed_precision_param: float16 # bfloat16, float32

train_dataset:
  name: HuggingFaceFW/fineweb-edu
  # name: c4
  stream: true
  seq_len: 2048
  batch_size: 48

val_dataset:
  # name: HuggingFaceFW/fineweb-edu
  name: c4
  stream: true
  seq_len: 2048
  batch_size: 1

metrics:
  enable_wb: false
  log_freq: 50
  project_name: fmengine-debug
  project_group: stf-h100
  project_id: stf-h100-testrun-acc4

profiling:
  profile_freq: 500

optimizer:
  name: adamw # adam, adamw
  lr: 5e-3
  fused: true

auto_patch:
  use_transformer_engine: false