model:
  architecture: llama
  attention_bias: false
  bos_token_id: 1
  eos_token_id: 2
  activation: silu
  hidden_size: 2048
  initializer_range: 0.02
  intermediate_size: 5632
  max_seq_len: 2048
  n_heads: 32
  n_layers: 24
  n_kv_heads: 4
  rms_norm_eps: 1.0e-05
  rope_theta: 500000
  tie_word_embeddings: false
  torch_dtype: bfloat16
  use_cache: true
  ffn_dim_multiplier: 1.3
  multiple_of: 1024
  vocab_size: 32000

tokenizer:
  tokenizer_type: huggingface
  tokenizer_name_or_path: meta-llama/Llama-2-7b-chat-hf

checkpoint:
  ckpt_dir: .local/ckpts2
  create_seed_checkpoint: false
  interval: 2000
  async_mode: disabled


training:
  gc_freq: 1000
  dp_degree: 2
  tp_degree: 1
  pp_degree: 1
  enable_loss_parallel: true
  data_parallel_type: fsdp
  dump_folder: ./outputs
  train_steps: 10000
  warmup_steps: 2000
  ac_mode: selective
  selective_ac_option: "2"
  compile: false

dataset:
  name: c4
  stream: true
  seq_len: 2048
  batch_size: 2

metrics:
  enable_wb: true
  log_freq: 1

profiling:
  profile_freq: 500

optimizer:
  name: adamw
  lr: 1e-3
  fused: true